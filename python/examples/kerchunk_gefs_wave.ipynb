{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GEFS Wave Global Collection\n",
    "\n",
    "Reads the Global GEFS Wave grib collection from NODD's S3 bucket and creates a single dataset using fsspec's ReferenceFileSystem\n",
    "\n",
    "This notebook demonstrates how to generate the reference JSON files using Kerchunk and gribberish. It was adapted from [this notebook](https://nbviewer.org/gist/peterm790/92eb1df3d58ba41d3411f8a840be2452)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fsspec"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the virtual filesystem from which to read the NODD s3 bucket. We are going to write the resulting dataset out to our local filesystem, so we leave the write system blank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs_read = fsspec.filesystem('s3', anon=True, skip_instance_cache=True, use_ssl=False) # For now SSL false is solving my cert issues **shrug**\n",
    "fs_write = fsspec.filesystem('')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, grab all of the files for the given model run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 105 GEFS files\n"
     ]
    }
   ],
   "source": [
    "#https://noaa-gefs-pds.s3.amazonaws.com/gefs.20230706/12/wave/gridded/gefs.wave.t12z.p01.global.0p25.f012.grib2\n",
    "gefs_ens_member_files = fs_read.glob('s3://noaa-gefs-pds/gefs.20230706/12/wave/gridded/gefs.wave.t12z.p01.global.0p25.*.grib2')\n",
    "\n",
    "files = sorted(['s3://'+f for f in gefs_ens_member_files])\n",
    "print(f'Read {len(files)} GEFS files')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each file, we will scan it with gribberish and extract its data to a dictionary formatted to the zarr spec. Then we will write that dictionary to a JSON file with the name formatting specified in the `make_json_name` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ujson\n",
    "from gribberish.kerchunk import scan_gribberish\n",
    "\n",
    "so = {\"anon\": True, \"use_ssl\": False}\n",
    "json_dir = 'gefs_wave/'\n",
    "\n",
    "def make_json_name(file_url, message_number): #create a unique name for each reference file\n",
    "    date = file_url.split('/')[3].split('.')[1]\n",
    "    name = file_url.split('/')[7].split('.')[0:5]\n",
    "    return f'{json_dir}{date}_{name[0]}_{name[1]}_{name[2]}_{name[4]}_message{message_number}.json'\n",
    "\n",
    "def gen_json(file_url):\n",
    "    print(f'Processing {file_url}...')\n",
    "    out = scan_gribberish(file_url, storage_options=so)   # create the reference using scan_grib\n",
    "    print(out)\n",
    "    for i, message in enumerate(out): # scan_grib outputs a list containing one reference per grib message\n",
    "        out_file_name = make_json_name(file_url, i)  # get name\n",
    "        with fs_write.open(out_file_name, \"w\") as f: \n",
    "            f.write(ujson.dumps(message)) # write to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing s3://noaa-gefs-pds/gefs.20230706/12/wave/gridded/gefs.wave.t12z.p01.global.0p25.f000.grib2...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "missing object_codec for object array",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:3\u001b[0m\n",
      "Cell \u001b[0;32mIn[4], line 14\u001b[0m, in \u001b[0;36mgen_json\u001b[0;34m(file_url)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgen_json\u001b[39m(file_url):\n\u001b[1;32m     13\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mProcessing \u001b[39m\u001b[39m{\u001b[39;00mfile_url\u001b[39m}\u001b[39;00m\u001b[39m...\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> 14\u001b[0m     out \u001b[39m=\u001b[39m scan_gribberish(file_url, storage_options\u001b[39m=\u001b[39;49mso)   \u001b[39m# create the reference using scan_grib\u001b[39;00m\n\u001b[1;32m     15\u001b[0m     \u001b[39mprint\u001b[39m(out)\n\u001b[1;32m     16\u001b[0m     \u001b[39mfor\u001b[39;00m i, message \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(out): \u001b[39m# scan_grib outputs a list containing one reference per grib message\u001b[39;00m\n",
      "File \u001b[0;32m~/Developer/gribberish/python/gribberish/kerchunk/mapper.py:145\u001b[0m, in \u001b[0;36mscan_gribberish\u001b[0;34m(url, common, storage_options, skip, only_vars)\u001b[0m\n\u001b[1;32m    141\u001b[0m z[var_name]\u001b[39m.\u001b[39mattrs[\u001b[39m\"\u001b[39m\u001b[39m_ARRAY_DIMENSIONS\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m dims\n\u001b[1;32m    143\u001b[0m \u001b[39mfor\u001b[39;00m coord_name, coord_data \u001b[39min\u001b[39;00m dataset[\u001b[39m'\u001b[39m\u001b[39mcoords\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mitems():\n\u001b[1;32m    144\u001b[0m     \u001b[39m# TODO: Prob dont store inline for non regular grids\u001b[39;00m\n\u001b[0;32m--> 145\u001b[0m     _store_array_inline(\n\u001b[1;32m    146\u001b[0m         store,\n\u001b[1;32m    147\u001b[0m         z,\n\u001b[1;32m    148\u001b[0m         np\u001b[39m.\u001b[39;49marray(coord_data[\u001b[39m'\u001b[39;49m\u001b[39mvalues\u001b[39;49m\u001b[39m'\u001b[39;49m]),\n\u001b[1;32m    149\u001b[0m         coord_name,\n\u001b[1;32m    150\u001b[0m         coord_data[\u001b[39m'\u001b[39;49m\u001b[39mattrs\u001b[39;49m\u001b[39m'\u001b[39;49m]\n\u001b[1;32m    151\u001b[0m     )\n\u001b[1;32m    152\u001b[0m     z[coord_name]\u001b[39m.\u001b[39mattrs[\u001b[39m\"\u001b[39m\u001b[39m_ARRAY_DIMENSIONS\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m dims\n\u001b[1;32m    154\u001b[0m out\u001b[39m.\u001b[39mappend(\n\u001b[1;32m    155\u001b[0m     {\n\u001b[1;32m    156\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mversion\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m1\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    159\u001b[0m     }\n\u001b[1;32m    160\u001b[0m )\n",
      "File \u001b[0;32m~/Developer/gribberish/python/gribberish/kerchunk/mapper.py:37\u001b[0m, in \u001b[0;36m_store_array_inline\u001b[0;34m(store, z, data, var, attr)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_store_array_inline\u001b[39m(store, z, data, var, attr):\n\u001b[1;32m     36\u001b[0m     shape \u001b[39m=\u001b[39m \u001b[39mtuple\u001b[39m(data\u001b[39m.\u001b[39mshape \u001b[39mor\u001b[39;00m ())\n\u001b[0;32m---> 37\u001b[0m     d \u001b[39m=\u001b[39m z\u001b[39m.\u001b[39;49mcreate_dataset(\n\u001b[1;32m     38\u001b[0m         name\u001b[39m=\u001b[39;49mvar,\n\u001b[1;32m     39\u001b[0m         shape\u001b[39m=\u001b[39;49mshape,\n\u001b[1;32m     40\u001b[0m         chunks\u001b[39m=\u001b[39;49mshape,\n\u001b[1;32m     41\u001b[0m         dtype\u001b[39m=\u001b[39;49mdata\u001b[39m.\u001b[39;49mdtype,\n\u001b[1;32m     42\u001b[0m         fill_value\u001b[39m=\u001b[39;49m\u001b[39m-\u001b[39;49m\u001b[39m9999.0\u001b[39;49m,\n\u001b[1;32m     43\u001b[0m         compressor\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m     44\u001b[0m     )\n\u001b[1;32m     45\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(data, \u001b[39m\"\u001b[39m\u001b[39mtobytes\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m     46\u001b[0m         b \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39mtobytes()\n",
      "File \u001b[0;32m~/Developer/gribberish/python/examples/env/lib/python3.9/site-packages/zarr/hierarchy.py:1043\u001b[0m, in \u001b[0;36mGroup.create_dataset\u001b[0;34m(self, name, **kwargs)\u001b[0m\n\u001b[1;32m    984\u001b[0m \u001b[39m\"\"\"Create an array.\u001b[39;00m\n\u001b[1;32m    985\u001b[0m \n\u001b[1;32m    986\u001b[0m \u001b[39mArrays are known as \"datasets\" in HDF5 terminology. For compatibility\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1039\u001b[0m \n\u001b[1;32m   1040\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1041\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mmode\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m kwargs\n\u001b[0;32m-> 1043\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_write_op(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_create_dataset_nosync, name, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Developer/gribberish/python/examples/env/lib/python3.9/site-packages/zarr/hierarchy.py:895\u001b[0m, in \u001b[0;36mGroup._write_op\u001b[0;34m(self, f, *args, **kwargs)\u001b[0m\n\u001b[1;32m    892\u001b[0m     lock \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_synchronizer[group_meta_key]\n\u001b[1;32m    894\u001b[0m \u001b[39mwith\u001b[39;00m lock:\n\u001b[0;32m--> 895\u001b[0m     \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Developer/gribberish/python/examples/env/lib/python3.9/site-packages/zarr/hierarchy.py:1056\u001b[0m, in \u001b[0;36mGroup._create_dataset_nosync\u001b[0;34m(self, name, data, **kwargs)\u001b[0m\n\u001b[1;32m   1054\u001b[0m \u001b[39m# create array\u001b[39;00m\n\u001b[1;32m   1055\u001b[0m \u001b[39mif\u001b[39;00m data \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1056\u001b[0m     a \u001b[39m=\u001b[39m create(store\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_store, path\u001b[39m=\u001b[39;49mpath, chunk_store\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_chunk_store,\n\u001b[1;32m   1057\u001b[0m                \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1059\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1060\u001b[0m     a \u001b[39m=\u001b[39m array(data, store\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_store, path\u001b[39m=\u001b[39mpath, chunk_store\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_chunk_store,\n\u001b[1;32m   1061\u001b[0m               \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/Developer/gribberish/python/examples/env/lib/python3.9/site-packages/zarr/creation.py:178\u001b[0m, in \u001b[0;36mcreate\u001b[0;34m(shape, chunks, dtype, compressor, fill_value, order, store, synchronizer, overwrite, path, chunk_store, filters, cache_metadata, cache_attrs, read_only, object_codec, dimension_separator, write_empty_chunks, zarr_version, meta_array, storage_transformers, **kwargs)\u001b[0m\n\u001b[1;32m    175\u001b[0m     path \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m/\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    177\u001b[0m \u001b[39m# initialize array metadata\u001b[39;00m\n\u001b[0;32m--> 178\u001b[0m init_array(store, shape\u001b[39m=\u001b[39;49mshape, chunks\u001b[39m=\u001b[39;49mchunks, dtype\u001b[39m=\u001b[39;49mdtype, compressor\u001b[39m=\u001b[39;49mcompressor,\n\u001b[1;32m    179\u001b[0m            fill_value\u001b[39m=\u001b[39;49mfill_value, order\u001b[39m=\u001b[39;49morder, overwrite\u001b[39m=\u001b[39;49moverwrite, path\u001b[39m=\u001b[39;49mpath,\n\u001b[1;32m    180\u001b[0m            chunk_store\u001b[39m=\u001b[39;49mchunk_store, filters\u001b[39m=\u001b[39;49mfilters, object_codec\u001b[39m=\u001b[39;49mobject_codec,\n\u001b[1;32m    181\u001b[0m            dimension_separator\u001b[39m=\u001b[39;49mdimension_separator, storage_transformers\u001b[39m=\u001b[39;49mstorage_transformers)\n\u001b[1;32m    183\u001b[0m \u001b[39m# instantiate array\u001b[39;00m\n\u001b[1;32m    184\u001b[0m z \u001b[39m=\u001b[39m Array(store, path\u001b[39m=\u001b[39mpath, chunk_store\u001b[39m=\u001b[39mchunk_store, synchronizer\u001b[39m=\u001b[39msynchronizer,\n\u001b[1;32m    185\u001b[0m           cache_metadata\u001b[39m=\u001b[39mcache_metadata, cache_attrs\u001b[39m=\u001b[39mcache_attrs, read_only\u001b[39m=\u001b[39mread_only,\n\u001b[1;32m    186\u001b[0m           write_empty_chunks\u001b[39m=\u001b[39mwrite_empty_chunks, meta_array\u001b[39m=\u001b[39mmeta_array)\n",
      "File \u001b[0;32m~/Developer/gribberish/python/examples/env/lib/python3.9/site-packages/zarr/storage.py:438\u001b[0m, in \u001b[0;36minit_array\u001b[0;34m(store, shape, chunks, dtype, compressor, fill_value, order, overwrite, path, chunk_store, filters, object_codec, dimension_separator, storage_transformers)\u001b[0m\n\u001b[1;32m    435\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m compressor:\n\u001b[1;32m    436\u001b[0m     \u001b[39m# compatibility with legacy tests using compressor=[]\u001b[39;00m\n\u001b[1;32m    437\u001b[0m     compressor \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 438\u001b[0m _init_array_metadata(store, shape\u001b[39m=\u001b[39;49mshape, chunks\u001b[39m=\u001b[39;49mchunks, dtype\u001b[39m=\u001b[39;49mdtype,\n\u001b[1;32m    439\u001b[0m                      compressor\u001b[39m=\u001b[39;49mcompressor, fill_value\u001b[39m=\u001b[39;49mfill_value,\n\u001b[1;32m    440\u001b[0m                      order\u001b[39m=\u001b[39;49morder, overwrite\u001b[39m=\u001b[39;49moverwrite, path\u001b[39m=\u001b[39;49mpath,\n\u001b[1;32m    441\u001b[0m                      chunk_store\u001b[39m=\u001b[39;49mchunk_store, filters\u001b[39m=\u001b[39;49mfilters,\n\u001b[1;32m    442\u001b[0m                      object_codec\u001b[39m=\u001b[39;49mobject_codec,\n\u001b[1;32m    443\u001b[0m                      dimension_separator\u001b[39m=\u001b[39;49mdimension_separator,\n\u001b[1;32m    444\u001b[0m                      storage_transformers\u001b[39m=\u001b[39;49mstorage_transformers)\n",
      "File \u001b[0;32m~/Developer/gribberish/python/examples/env/lib/python3.9/site-packages/zarr/storage.py:559\u001b[0m, in \u001b[0;36m_init_array_metadata\u001b[0;34m(store, shape, chunks, dtype, compressor, fill_value, order, overwrite, path, chunk_store, filters, object_codec, dimension_separator, storage_transformers)\u001b[0m\n\u001b[1;32m    556\u001b[0m \u001b[39mif\u001b[39;00m object_codec \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    557\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m filters:\n\u001b[1;32m    558\u001b[0m         \u001b[39m# there are no filters so we can be sure there is no object codec\u001b[39;00m\n\u001b[0;32m--> 559\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mmissing object_codec for object array\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    560\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    561\u001b[0m         \u001b[39m# one of the filters may be an object codec, issue a warning rather\u001b[39;00m\n\u001b[1;32m    562\u001b[0m         \u001b[39m# than raise an error to maintain backwards-compatibility\u001b[39;00m\n\u001b[1;32m    563\u001b[0m         warnings\u001b[39m.\u001b[39mwarn(\u001b[39m'\u001b[39m\u001b[39mmissing object_codec for object array; this will raise a \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    564\u001b[0m                       \u001b[39m'\u001b[39m\u001b[39mValueError in version 3.0\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mFutureWarning\u001b[39;00m)\n",
      "\u001b[0;31mValueError\u001b[0m: missing object_codec for object array"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#this step is best run via a cluster\n",
    "for f in files[0:1]:\n",
    "    gen_json(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
